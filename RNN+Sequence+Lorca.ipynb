{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "from itertools import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 82702 characters, 72 unique.\n"
     ]
    }
   ],
   "source": [
    "data_file = '../../data/lorca.txt'\n",
    "\n",
    "data = codecs.open(data_file, 'rU', 'utf-8').read() # AH: Included unicode support through module 'codecs'\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'Data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "seq_length = 10 # number of steps to unroll the RNN for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "n_max = 100000\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "while n <= n_max:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        p = 0 # go from start of data\n",
    "    input = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    target = [char_to_ix[ch] for ch in data[p+seq_length:p+seq_length+1]]\n",
    "    p += 1\n",
    "    n += 1\n",
    "    inputs.append(input)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Targets to one hot encoding\n",
    "targets_array = []\n",
    "for target in targets:\n",
    "    _target = target[0]\n",
    "    target_array = np.zeros(72) # AH: OJO, esto debería ser vocab_size\n",
    "    target_array[_target] = 1\n",
    "    targets_array.append(target_array)\n",
    "targets = np.array(targets_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs to one hot encoding\n",
    "inputs_arrays = []\n",
    "for input in inputs:\n",
    "    input_array = []\n",
    "    for input_value in input:\n",
    "        input_ohv = np.zeros(72) # AH: OJO, esto debería ser vocab_size\n",
    "        input_ohv[input_value] = 1\n",
    "        input_array.append(input_ohv)\n",
    "    input_array = np.array(input_array)\n",
    "    inputs_arrays.append(input_array)\n",
    "inputs = np.array(inputs_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 10, 72)\n",
      "(100001, 72)\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array(inputs)\n",
    "output_data = np.array(targets)\n",
    "\n",
    "print input_data.shape\n",
    "print output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 10, 72)\n",
      "(100001, 72)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data from rows to columns\n",
    "# AH: No estoy seguro de lo que hace esto; parecen quedar con igual shape.\n",
    "input_data = input_data.reshape(input_data.shape[0], seq_length, 72)\n",
    "output_data = output_data.reshape(output_data.shape[0], 72)\n",
    "\n",
    "print input_data.shape\n",
    "print output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Split\n",
    "# AH: con LIMIT a 800 se pierde mucho dato de entrenamiento en sets grandes\n",
    "# AH: se podría probar con el 80% de input_data.shape[0].\n",
    "LIMIT = 800\n",
    "X_test = input_data[LIMIT:]\n",
    "Y_test = output_data[LIMIT:]\n",
    "\n",
    "X_train = input_data[:LIMIT]\n",
    "Y_train = output_data[:LIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fc71bd02e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "# Network variables\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, seq_length, 72])\n",
    "target = tf.placeholder(tf.float32, [None, 72])\n",
    "\n",
    "num_hidden = 2\n",
    "num_neurons = 50\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_neurons, state_is_tuple=False)\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([cell] * num_hidden, state_is_tuple=False)\n",
    "val, state = tf.nn.dynamic_rnn(stacked_lstm, data, dtype=tf.float32)\n",
    "\n",
    "# TODO: Understand what happens in the next couple of lines!!!\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([num_neurons, int(target.get_shape()[1])]))\n",
    "bias = tf.Variable(tf.constant(0.0, shape=[target.get_shape()[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(last, weight) + bias\n",
    "prediction = tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Functions definition \n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, target))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "minimize = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "no_of_batches = int(len(X_train) / batch_size)\n",
    "epochs = 4000\n",
    "\n",
    "loss_train_array = []\n",
    "train_accuracy_array = []\n",
    "test_accuracy_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000, loss train 3.96834111, train accuracy 0.08500000, test accuracy 0.09555347\n",
      "Epoch 0100, loss train 0.01597336, train accuracy 0.99250001, test accuracy 0.11893025\n",
      "Epoch 0200, loss train 0.01089315, train accuracy 0.99250001, test accuracy 0.11709560\n",
      "Epoch 0300, loss train 0.00988228, train accuracy 0.99250001, test accuracy 0.11587585\n",
      "Epoch 0400, loss train 0.00948339, train accuracy 0.99250001, test accuracy 0.11559359\n",
      "Epoch 0500, loss train 0.00927098, train accuracy 0.99250001, test accuracy 0.11494844\n",
      "Epoch 0600, loss train 0.00914481, train accuracy 0.99250001, test accuracy 0.11431336\n",
      "Epoch 0700, loss train 0.00906032, train accuracy 0.99250001, test accuracy 0.11415207\n",
      "Epoch 0800, loss train 0.00900305, train accuracy 0.99250001, test accuracy 0.11337587\n",
      "Epoch 0900, loss train 0.00895426, train accuracy 0.99250001, test accuracy 0.11324482\n",
      "Epoch 1000, loss train 0.00891881, train accuracy 0.99250001, test accuracy 0.11293233\n",
      "Epoch 1100, loss train 0.00889281, train accuracy 0.99250001, test accuracy 0.11284161\n",
      "Epoch 1200, loss train 0.00886818, train accuracy 0.99250001, test accuracy 0.11254927\n",
      "Epoch 1300, loss train 0.00884957, train accuracy 0.99250001, test accuracy 0.11231741\n",
      "Epoch 1400, loss train 0.00883295, train accuracy 0.99250001, test accuracy 0.11201500\n",
      "Epoch 1500, loss train 0.00882096, train accuracy 0.99250001, test accuracy 0.11195452\n",
      "Epoch 1600, loss train 0.00881020, train accuracy 0.99250001, test accuracy 0.11199484\n",
      "Epoch 1700, loss train 0.00880101, train accuracy 0.99250001, test accuracy 0.11191420\n",
      "Epoch 1800, loss train 0.00880748, train accuracy 0.99250001, test accuracy 0.11160170\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        ptr = 0\n",
    "        for j in range(no_of_batches):\n",
    "            inp, out = X_train[ptr:ptr + batch_size], Y_train[ptr:ptr + batch_size]\n",
    "            ptr += batch_size\n",
    "            _, loss_train = sess.run([minimize, loss], {data: inp, target: out})\n",
    "        \n",
    "\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(\n",
    "                accuracy, feed_dict={data: X_train, target: Y_train})\n",
    "            train_accuracy_array.append(train_accuracy)\n",
    "\n",
    "            test_accuracy = sess.run(\n",
    "                accuracy, feed_dict={data: X_test, target: Y_test})\n",
    "            test_accuracy_array.append(test_accuracy)\n",
    "\n",
    "            print (\n",
    "                'Epoch %04d, '\n",
    "                'loss train %.8f, train accuracy %.8f, test accuracy %.8f'\n",
    "                %\n",
    "                (i,\n",
    "                 loss_train,\n",
    "                 train_accuracy,\n",
    "                 test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample from the model now and then\n",
    "preds = []\n",
    "ix = X_train[:1]\n",
    "\n",
    "for i in range(1000):\n",
    "    pred = sess.run(prediction ,{data: ix.reshape(1, seq_length, 72)})\n",
    "    index = np.argmax(pred, 1)\n",
    "    \n",
    "    preds.append(ix_to_char[index[0]])\n",
    "    \n",
    "    zero_array = np.zeros(72)\n",
    "    zero_array[index] = 1\n",
    "    \n",
    "    tail = np.append(np.array(ix[0][1:]), np.array(zero_array.reshape(1, 72)))\n",
    "    \n",
    "    ix = tail.reshape(1, seq_length, 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veleta\n",
      "\n",
      "Viento del Sur,\n",
      "moreno, ardiente,\n",
      "llegas sobre mi carne\n",
      "tembloroso de auroras\n",
      "boreales,\n",
      "con tu capa de espectros\n",
      "capitanes,\n",
      "y riyéndote a gritos\n",
      "del Dante.\n",
      "¡Oh pulidor de estrellas!\n",
      "Pero vienes\n",
      "demasiado tarde.\n",
      "Mi almario está musgoso\n",
      "y he perdido la llave.\n",
      "\n",
      "Sin ningún viento,\n",
      "¡hazme caso!,\n",
      "gira, corazón.\n",
      "\n",
      "Brisas, gnomos y vientos\n",
      "de ninguna parte.\n",
      "Mosquitos de la rosa\n",
      "de pétalos pirámides.\n",
      "Alisios destetados\n",
      "entre los rudos árboles,\n",
      "flautass,\n",
      "ydcae  cojatpa ddem   lasla\n",
      "de bidalantdeshfelal\n",
      "tasbsrmm  vanoo\n",
      "tvoo n iree,\n",
      "¡oéhV  rosdca rdstas\n",
      "\n",
      "memapidonrád blósr\n",
      "llalársblslml cientes\n",
      "denntusgo as e rarlos,\n",
      "deraétodos dne turos!\n",
      "goora irne \n",
      "el leas,bnmrrodo e arluras\n",
      "boemlrasoo  e iuit,\n",
      "eo hzranas\n",
      "e!b,imrast,,cerdn, ryyho posdendálaalslllal pirádees\n",
      ".blriies.\n",
      "tdlmesss brosoros á iumoscasarte,sdpadbula roa\n",
      "e brelasos\n",
      "detaide talmrdss\n",
      "ddal eetat\n",
      "lncone iejlét eusuayso,!gorana  aolla e.\n",
      "Silnvntre MMerda de dulla ase.\n",
      "Piresd, ppiaiios,b ipo ard\n",
      " te e!\n",
      "llas nn gurossce de tacase\n",
      "de ppa\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
